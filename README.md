# Atom Eval

åŸºäº [EvalScope](https://github.com/modelscope/evalscope) çš„ LLM è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡å’Œæ¨¡å‹çš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚

## é¡¹ç›®ä»‹ç»

Atom Eval æ˜¯ä¸€ä¸ªçµæ´»ã€å¯æ‰©å±•çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©ç ”ç©¶è€…å’Œå¼€å‘è€…å¿«é€Ÿæ„å»ºå’Œè¿è¡Œè‡ªå®šä¹‰çš„è¯„ä¼°ä»»åŠ¡ã€‚é¡¹ç›®åŸºäº EvalScope æ„å»ºï¼Œæä¾›äº†å®Œæ•´çš„è¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹è°ƒç”¨ã€ç»“æœè¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ¯ **å¤šä»»åŠ¡æ”¯æŒ**ï¼šå†…ç½®å¤šç§è¯„ä¼°ä»»åŠ¡ï¼Œæ”¯æŒå¿«é€Ÿæ‰©å±•
- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒ DeepSeekã€Qwen ç­‰å¤šä¸ªä¸»æµ LLM æ¨¡å‹
- ğŸ“Š **çµæ´»é…ç½®**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶è½»æ¾ç®¡ç†æ¨¡å‹å’Œæ•°æ®é›†
- ğŸ“ˆ **è¯¦ç»†æŠ¥å‘Š**ï¼šè‡ªåŠ¨ç”Ÿæˆè¯„ä¼°ç»“æœã€æ—¥å¿—å’Œå¯è§†åŒ–æŠ¥å‘Š
- ğŸ”§ **æ˜“äºæ‰©å±•**ï¼šæ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œä¾¿äºæ·»åŠ æ–°çš„è¯„ä¼°ä»»åŠ¡å’Œæ¨¡å‹

## ä¸ºä»€ä¹ˆéœ€è¦è‡ªå®šä¹‰ Benchmark è¯„æµ‹ï¼Ÿ

### 1. é¢†åŸŸç‰¹å®šéœ€æ±‚

é€šç”¨è¯„ä¼°æ•°æ®é›†ï¼ˆå¦‚ MMLUã€HellaSwag ç­‰ï¼‰è™½ç„¶è¦†ç›–é¢å¹¿ï¼Œä½†å¾€å¾€æ— æ³•æ»¡è¶³ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°éœ€æ±‚ã€‚ä¾‹å¦‚ï¼š
- **ä¸šåŠ¡åœºæ™¯**ï¼šéœ€è¦è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šä¸šåŠ¡é€»è¾‘ä¸‹çš„è¡¨ç°
- **è¡Œä¸šæ ‡å‡†**ï¼šéœ€è¦ç¬¦åˆç‰¹å®šè¡Œä¸šçš„æ ‡å‡†å’Œè§„èŒƒ
- **æœ¬åœ°åŒ–éœ€æ±‚**ï¼šéœ€è¦è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šè¯­è¨€ã€æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°

### 2. æ•°æ®éšç§å’Œå®‰å…¨

ä½¿ç”¨è‡ªå®šä¹‰ benchmark å¯ä»¥ï¼š
- é¿å…å°†æ•æ„Ÿæ•°æ®ä¸Šä¼ åˆ°å…¬å…±å¹³å°
- åœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿›è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹
- æ§åˆ¶æ•°æ®çš„è®¿é—®å’Œä½¿ç”¨æƒé™

### 3. è¯„ä¼°æŒ‡æ ‡å®šåˆ¶

è‡ªå®šä¹‰ benchmark å…è®¸ï¼š
- å®šä¹‰ç¬¦åˆä¸šåŠ¡éœ€æ±‚çš„è¯„ä¼°æŒ‡æ ‡
- å®ç°é¢†åŸŸç‰¹å®šçš„è¯„ä¼°é€»è¾‘
- çµæ´»è°ƒæ•´è¯„ä¼°æ ‡å‡†å’Œæƒé‡

### 4. æŒç»­æ”¹è¿›å’Œè¿­ä»£

è‡ªå®šä¹‰ benchmark æ”¯æŒï¼š
- æ ¹æ®æ¨¡å‹è¡¨ç°æŒç»­ä¼˜åŒ–è¯„ä¼°ä»»åŠ¡
- å¿«é€Ÿæ·»åŠ æ–°çš„æµ‹è¯•ç”¨ä¾‹
- è·Ÿè¸ªæ¨¡å‹åœ¨ä¸åŒç‰ˆæœ¬é—´çš„æ€§èƒ½å˜åŒ–

## ä¾èµ–å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python >= 3.8
- pip

### å®‰è£…æ­¥éª¤

1. å…‹éš†é¡¹ç›®ï¼š

```bash
git clone https://github.com/shaoeric/atom_eval.git
cd atom_eval
```

2. å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirement.txt
```

### ä¸»è¦ä¾èµ–

- `evalscope==1.4.1`: è¯„ä¼°æ¡†æ¶æ ¸å¿ƒåº“
- `openai==2.15.0`: OpenAI API å…¼å®¹æ¥å£
- `python-dotenv==1.2.1`: ç¯å¢ƒå˜é‡ç®¡ç†
- `datasets==3.6.0`: æ•°æ®é›†å¤„ç†
- `pandas==2.3.3`: æ•°æ®å¤„ç†
- `numpy==2.4.1`: æ•°å€¼è®¡ç®—

å®Œæ•´ä¾èµ–åˆ—è¡¨è¯·å‚è€ƒ `requirement.txt`ã€‚

## æ”¯æŒçš„ä»»åŠ¡

Atom Eval ç›®å‰æ”¯æŒä»¥ä¸‹è¯„ä¼°ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰è¯¦ç»†çš„ README æ–‡æ¡£ï¼š

### 1. General QAï¼ˆé€šç”¨é—®ç­”ï¼‰

è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ”¯æŒå¤šç§é—®ç­”æ ¼å¼ã€‚

- **ä»»åŠ¡æè¿°**ï¼šæµ‹è¯•æ¨¡å‹å›ç­”é—®é¢˜çš„èƒ½åŠ›
- **è¯„ä¼°æŒ‡æ ‡**ï¼šBLEU-1/2/3ã€ROUGE ç³»åˆ—
- **è¯¦ç»†æ–‡æ¡£**ï¼š[benchmarks/general_qa/README.md](benchmarks/general_qa/README.md)

### 2. Text2SQLï¼ˆæ–‡æœ¬è½¬SQLï¼‰

è¯„ä¼°æ¨¡å‹å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸º SQL æŸ¥è¯¢è¯­å¥çš„èƒ½åŠ›ã€‚

- **ä»»åŠ¡æè¿°**ï¼šæµ‹è¯•æ¨¡å‹ç†è§£è‡ªç„¶è¯­è¨€é—®é¢˜ã€æ•°æ®åº“ç»“æ„å’Œç”Ÿæˆ SQL çš„èƒ½åŠ›
- **è¯„ä¼°æŒ‡æ ‡**ï¼šSQL AST ç›¸ä¼¼åº¦ï¼ˆsql_ast_simï¼‰
- **è¯¦ç»†æ–‡æ¡£**ï¼š[benchmarks/text2sql/README.md](benchmarks/text2sql/README.md)

### 3. Function Callï¼ˆå‡½æ•°è°ƒç”¨ï¼‰

è¯„ä¼°æ¨¡å‹åœ¨å‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

- **ä»»åŠ¡æè¿°**ï¼šæµ‹è¯•æ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å‡½æ•°ã€é€‰æ‹©æ­£ç¡®å‡½æ•°å’Œç”Ÿæˆå‚æ•°çš„èƒ½åŠ›
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå·¥å…·è°ƒç”¨å‡†ç¡®ç‡ã€æˆåŠŸè°ƒç”¨æ¬¡æ•°
- **è¯¦ç»†æ–‡æ¡£**ï¼š[benchmarks/function_call/README.md](benchmarks/function_call/README.md)

### 4. HaluEvalï¼ˆå¹»è§‰æ£€æµ‹ï¼‰

è¯„ä¼°æ¨¡å‹è¯†åˆ«å¹»è§‰ï¼ˆHallucinationï¼‰çš„èƒ½åŠ›ã€‚

- **ä»»åŠ¡æè¿°**ï¼šæµ‹è¯•æ¨¡å‹åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸çŸ¥è¯†åº“ä¸ä¸€è‡´ä¿¡æ¯çš„èƒ½åŠ›
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ã€ç²¾ç¡®ç‡ï¼ˆprecisionï¼‰ã€å¬å›ç‡ï¼ˆrecallï¼‰ã€F1 åˆ†æ•°ã€è‚¯å®šæ¯”ä¾‹ï¼ˆyes_ratioï¼‰
- **å­ä»»åŠ¡**ï¼šå¯¹è¯æ ·æœ¬ã€é—®ç­”æ ·æœ¬ã€æ‘˜è¦æ ·æœ¬
- **è¯¦ç»†æ–‡æ¡£**ï¼š[benchmarks/halu_eval/README.md](benchmarks/halu_eval/README.md)

### 5. FRAMESï¼ˆRAG ç³»ç»Ÿè¯„ä¼°ï¼‰

è¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„ç»¼åˆèƒ½åŠ›ã€‚

- **ä»»åŠ¡æè¿°**ï¼šæµ‹è¯• RAG ç³»ç»Ÿåœ¨äº‹å®æ€§ã€æ£€ç´¢å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ï¼ˆaccï¼‰ï¼Œæ”¯æŒæ ‡å‡†è¯„ä¼°å’Œ LLM Judge è¯„ä¼°
- **è¯¦ç»†æ–‡æ¡£**ï¼š[benchmarks/frames/README.md](benchmarks/frames/README.md)

## è‡ªå®šä¹‰æ¨¡å‹é…ç½®

Atom Eval æ”¯æŒæ·»åŠ è‡ªå®šä¹‰æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚è¯¦ç»†é…ç½®è¯´æ˜è¯·å‚è€ƒï¼š

ğŸ“– [è‡ªå®šä¹‰æ¨¡å‹é…ç½®æ–‡æ¡£](docs/custom_model.md)


## è‡ªå®šä¹‰ Benchmark é…ç½®

Atom Eval æ”¯æŒåˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°ä»»åŠ¡ï¼ˆBenchmarkï¼‰ã€‚è¯¦ç»†é…ç½®è¯´æ˜è¯·å‚è€ƒï¼š

ğŸ“– [è‡ªå®šä¹‰ Benchmark é…ç½®æ–‡æ¡£](docs/custom_benchmark.md)


## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirement.txt

# é…ç½®ç¯å¢ƒå˜é‡
cp .env_example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å†™ API å¯†é’¥ç­‰ä¿¡æ¯
```

### 2. è¿è¡Œè¯„ä¼°

```bash
# è¿è¡Œ General QA è¯„ä¼°
python benchmarks/general_qa/main.py --model deepseek-chat

# è¿è¡Œ Text2SQL è¯„ä¼°
python benchmarks/text2sql/main.py --model Qwen/Qwen3-Next-80B-A3B-Instruct-FP8

# è¿è¡Œ Function Call è¯„ä¼°
python benchmarks/function_call/main.py --model deepseek-chat

# è¿è¡Œ HaluEval è¯„ä¼°
python benchmarks/halu_eval/main.py --model deepseek-chat

# è¿è¡Œ FRAMES è¯„ä¼°ï¼ˆä½¿ç”¨ LLM Judgeï¼‰
python benchmarks/frames/main.py --model deepseek-chat --use_llm_judge --judge_model_name deepseek-reasoner
```

### 3. å‘½ä»¤è¡Œå‚æ•°

æ‰€æœ‰ benchmark æ”¯æŒä»¥ä¸‹å‚æ•°ï¼š

- `--model`: æ¨¡å‹åç§°ï¼ˆå¿…é€‰ï¼Œæˆ–é€šè¿‡ç¯å¢ƒå˜é‡ `USE_LLM_NAME` è®¾ç½®ï¼‰
- `--dataset`: æ•°æ®é›†åç§°ï¼ˆé»˜è®¤ä¸ benchmark åç§°ç›¸åŒï¼‰
- `--batch_size`: æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤ï¼š1ï¼‰
- `--max_tokens`: æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ï¼š2048ï¼‰
- `--limit`: é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ˆå¯é€‰ï¼‰
- `--use_llm_judge`: æ˜¯å¦ä½¿ç”¨ LLM Judge è¯„ä¼°ï¼ˆéƒ¨åˆ† benchmark æ”¯æŒï¼‰
- `--judge_model_name`: LLM Judge æ¨¡å‹åç§°ï¼ˆä½¿ç”¨ `--use_llm_judge` æ—¶å¿…é€‰ï¼‰

ç¤ºä¾‹ï¼š

```bash
# é™åˆ¶è¯„ä¼°å‰ 10 ä¸ªæ ·æœ¬
python benchmarks/text2sql/main.py --model deepseek-chat --limit 10

# è‡ªå®šä¹‰æ‰¹é‡å¤§å°å’Œæœ€å¤§ token æ•°
python benchmarks/text2sql/main.py --model deepseek-chat --batch_size 4 --max_tokens 4096
```

## é¡¹ç›®ç»“æ„

```
atom_eval/
â”œâ”€â”€ benchmarks/              # è¯„ä¼°ä»»åŠ¡å®ç°
â”‚   â”œâ”€â”€ general_qa/         # é€šç”¨é—®ç­”ä»»åŠ¡
â”‚   â”œâ”€â”€ text2sql/          # Text2SQL ä»»åŠ¡
â”‚   â”œâ”€â”€ function_call/     # å‡½æ•°è°ƒç”¨ä»»åŠ¡
â”‚   â”œâ”€â”€ halu_eval/         # å¹»è§‰æ£€æµ‹ä»»åŠ¡
â”‚   â””â”€â”€ frames/            # FRAMES RAG è¯„ä¼°ä»»åŠ¡
â”œâ”€â”€ datasets/              # æ•°æ®é›†ç›®å½•
â”‚   â””â”€â”€ llm/              # LLM æ•°æ®é›†
â”‚       â”œâ”€â”€ qa/          # é—®ç­”æ•°æ®é›†
â”‚       â”œâ”€â”€ text2sql/     # Text2SQL æ•°æ®é›†
â”‚       â”œâ”€â”€ function_call/ # å‡½æ•°è°ƒç”¨æ•°æ®é›†
â”‚       â”œâ”€â”€ halueval/     # HaluEval æ•°æ®é›†
â”‚       â””â”€â”€ frames/       # FRAMES æ•°æ®é›†
â”œâ”€â”€ results/              # è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
â”‚   â””â”€â”€ {benchmark_name}/ # å„ä»»åŠ¡çš„è¯„ä¼°ç»“æœ
â”‚       â””â”€â”€ {model_name}_{params}/
â”‚           â”œâ”€â”€ reviews/  # è¯¦ç»†è¯„ä¼°ç»“æœ
â”‚           â””â”€â”€ reports/  # è¯„ä¼°æŠ¥å‘Š
â”œâ”€â”€ docs/                 # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ custom_model.md  # è‡ªå®šä¹‰æ¨¡å‹é…ç½®æ–‡æ¡£
â”‚   â””â”€â”€ custom_benchmark.md # è‡ªå®šä¹‰ Benchmark é…ç½®æ–‡æ¡£
â”œâ”€â”€ config.py             # é…ç½®æ–‡ä»¶
â”œâ”€â”€ utils.py              # å·¥å…·å‡½æ•°
â”œâ”€â”€ requirement.txt       # Python ä¾èµ–
â””â”€â”€ .env_example         # ç¯å¢ƒå˜é‡ç¤ºä¾‹
```

## è¯„ä¼°ç»“æœ

è¯„ä¼°ç»“æœä¿å­˜åœ¨ `results/{benchmark_name}/{model_name}_{params}/` ç›®å½•ä¸‹ï¼ŒåŒ…æ‹¬ï¼š

- `reviews/`: æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†è¯„ä¼°ç»“æœï¼ˆJSONL æ ¼å¼ï¼‰
- `reports/`: æ±‡æ€»è¯„ä¼°æŠ¥å‘Šï¼ˆJSON æ ¼å¼ï¼‰
- `logs/`: è¯„ä¼°æ—¥å¿—æ–‡ä»¶

## æ›´æ–°æ—¥å¿—

è¯¦ç»†çš„ç‰ˆæœ¬æ›´æ–°å’Œå˜æ›´è®°å½•è¯·æŸ¥çœ‹ï¼š

ğŸ“‹ [RELEASE_NOTES.md](RELEASE_NOTES.md)

## è®¸å¯è¯

è¯·æŸ¥çœ‹é¡¹ç›®æ ¹ç›®å½•çš„ LICENSE æ–‡ä»¶ã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ç›¸å…³é“¾æ¥

- [EvalScope](https://github.com/modelscope/evalscope): åº•å±‚è¯„ä¼°æ¡†æ¶
- [ModelScope](https://www.modelscope.cn/): æ¨¡å‹å’Œæ•°æ®é›†å¹³å°
